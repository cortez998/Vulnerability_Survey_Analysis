#summary of the NAs of the variables and basic statistics
import pandas as pd

def summarize_frame(df: pd.DaDataFrame) -> pd.DataFrame:
    dataframe = pd.DataFrame()
    
    dataframe['dtype'] = df.dtypes
    
    dataframe['first_value'] = df.iloc[0]
    
    nan_perc = []
    for i in df.columns:
        nan_count = df[i].isna().sum()
        nan_perc.append(nan_count/len(df[i]))
    dataframe['nan_perc'] = nan_perc
    
    unique_count = []
    for i in df.columns:
        unique_count.append(len(df[i].dropna().unique()))
    dataframe['unique_count'] = unique_count
    
    dataframe['mean'] = df.mean()
    
    dataframe['st_dev'] = df.std()
    
    return dataframe


#define treshold for bad columns

import pandas as pd
from typing import List

def get_bad_columns(df: pd.DataFrame, p: float) -> List[str]:
    bad_columns = []
    for i in df.columns:
        perc_nan_sum = df[i].isna().sum()/len(df[i])
        if perc_nan_sum > p:
            bad_columns.append(i)
    return bad_columns


#RANDOM FOREST
#data
import pandas as pd
import numpy as np
from sklearn.utils import shuffle 
data = shuffle(pd.read_csv("data.csv"), random_state= 2020)
X_train = data.iloc[:18000, :2]; y_train = data.iloc[:18000, 2:]
X_test = data.iloc[18000:, :2]; y_test = data.iloc [18000:, 2:]

#random forest model
from sklearn import ensemble
rf = ensemble.RandomForestRegressor(n_estimators = 30, min_samples_leaf=300, random_state=1970)

#train the random forest model
rf.fit(X_train, y_train)

#compute the prediction over the training and testing sets
y_pred_train = rf.predict(X_train)
y_pred_test = rf.predict(X_test)

#compute the MSE for the training and testing sets
from sklearn.metrics import mean_squared_error

error_train = mean_squared_error(y_train, y_pred_train)
error_test = mean_squared_error(y_test, y_pred_test)

print("MSE on training set:", error_train)
print("MSE on testing set:", error_test)

#get the name and the importance measure of the variable
#with the highest importance measure

features = list(X_train)
importances = rf.feature_importances_
indices = np.flip(np.argsort(importances))

name = features[indices[0]]
importance = np.max(rf.feature_importances_)

print("Name of the variable with the highest importance measure:", name)
print("Corresponding importance measure:", importance)


#BOOSTING
#data
import pandas as pd
import numpy as np
from sklearn.utils import shuffle 
data = shuffle(pd.read_csv("data.csv"), random_state= 2020)
X_train = data.iloc[:18000, :2]; y_train = data.iloc[:18000, 2:]
X_test = data.iloc[18000:, :2]; y_test = data.iloc [18000:, 2:]

#gradient boosting model
from sklearn.ensemble import GradientBoostingRegressor
gbm = GradientBoostingRegressor(loss = 'ls', criterion = "mse", learning_rate = 0.9,
                                n_estimators = 500, min_samples_leaf = 100,
                                random_state = 1970, verbose = 0)

#train the gradient boosting model
gbm.fit(X_train, y_train)

#compute the prediction over the training and testing sets
y_pred_train = gbm.predict(X_train)
y_pred_test = gbm.predict(X_test)

#compute the MSE for the training and testing sets
from sklearn.metrics import mean_squared_error 

error_train = mean_squared_error(y_train, y_pred_train)
error_test = mean_squared_error(y_test, y_pred_test)

print("MSE on training set:", error_train)
print("MSE on testing set:", error_test)

#get the name and the importance measure of the variable
#with the highest importance measure
features = list(X_train)
importances = gbm.feature_importances_ 
indices = np.flip(np.argsort(importances))

name = features[indices[0]]
importance = np.max(gbm.feature_importances_)

print("Name of the variable with the highest importance measure:", name)
print("Corresponding importance measure:", importance)




